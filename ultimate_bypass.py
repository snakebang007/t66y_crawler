#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»ˆæåçˆ¬è™«ç ´è§£å™¨
ä¸“é—¨é’ˆå¯¹ç‰¹æ®Šçš„åçˆ¬è™«æœºåˆ¶
"""

import requests
import time
import random
import json
import base64
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import re
import sys
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class UltimateBypass:
    """ç»ˆæç»•è¿‡å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        
    def try_special_methods(self, url):
        """å°è¯•ç‰¹æ®Šæ–¹æ³•"""
        print(f"ğŸ¯ å¼€å§‹ç»ˆæç ´è§£: {url}")
        print("=" * 60)
        
        methods = [
            self.method_old_browser,
            self.method_wget_simulation,
            self.method_python_requests,
            self.method_api_client,
            self.method_rss_reader,
            self.method_social_media_bot,
            self.method_archive_crawler,
            self.method_academic_crawler
        ]
        
        for i, method in enumerate(methods, 1):
            print(f"\nğŸ”§ å°è¯•ç‰¹æ®Šæ–¹æ³• {i}/{len(methods)}")
            try:
                result = method(url)
                if result and self.is_success(result):
                    print(f"ğŸ‰ æ–¹æ³• {i} æˆåŠŸç ´è§£!")
                    return result
                else:
                    print(f"âŒ æ–¹æ³• {i} å¤±è´¥")
            except Exception as e:
                print(f"âŒ æ–¹æ³• {i} å¼‚å¸¸: {e}")
            
            time.sleep(random.uniform(1, 2))
        
        return None
    
    def method_old_browser(self, url):
        """æ–¹æ³•1: æ¨¡æ‹Ÿè€ç‰ˆæœ¬æµè§ˆå™¨"""
        print("ğŸ”„ æ¨¡æ‹Ÿè€ç‰ˆæœ¬æµè§ˆå™¨")
        headers = {
            'User-Agent': 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-us,en;q=0.5',
            'Accept-Encoding': 'gzip,deflate',
            'Connection': 'keep-alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "è€ç‰ˆæœ¬æµè§ˆå™¨")
    
    def method_wget_simulation(self, url):
        """æ–¹æ³•2: æ¨¡æ‹Ÿwget"""
        print("ğŸ”„ æ¨¡æ‹Ÿwgetå·¥å…·")
        headers = {
            'User-Agent': 'Wget/1.21.3',
            'Accept': '*/*',
            'Accept-Encoding': 'identity',
            'Connection': 'Keep-Alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "wgetæ¨¡æ‹Ÿ")
    
    def method_python_requests(self, url):
        """æ–¹æ³•3: æ¨¡æ‹ŸPython requestsåº“"""
        print("ğŸ”„ æ¨¡æ‹ŸPython requests")
        headers = {
            'User-Agent': 'python-requests/2.31.0',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "Python requests")
    
    def method_api_client(self, url):
        """æ–¹æ³•4: æ¨¡æ‹ŸAPIå®¢æˆ·ç«¯"""
        print("ğŸ”„ æ¨¡æ‹ŸAPIå®¢æˆ·ç«¯")
        headers = {
            'User-Agent': 'PostmanRuntime/7.32.3',
            'Accept': '*/*',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "APIå®¢æˆ·ç«¯")
    
    def method_rss_reader(self, url):
        """æ–¹æ³•5: æ¨¡æ‹ŸRSSé˜…è¯»å™¨"""
        print("ğŸ”„ æ¨¡æ‹ŸRSSé˜…è¯»å™¨")
        headers = {
            'User-Agent': 'FeedBurner/1.0 (http://www.FeedBurner.com)',
            'Accept': 'application/rss+xml, application/xml, text/xml',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "RSSé˜…è¯»å™¨")
    
    def method_social_media_bot(self, url):
        """æ–¹æ³•6: æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“çˆ¬è™«"""
        print("ğŸ”„ æ¨¡æ‹Ÿç¤¾äº¤åª’ä½“çˆ¬è™«")
        bots = [
            'facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)',
            'Twitterbot/1.0',
            'LinkedInBot/1.0 (compatible; Mozilla/5.0; Apache-HttpClient +http://www.linkedin.com/)'
        ]
        headers = {
            'User-Agent': random.choice(bots),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "ç¤¾äº¤åª’ä½“çˆ¬è™«")
    
    def method_archive_crawler(self, url):
        """æ–¹æ³•7: æ¨¡æ‹Ÿå­˜æ¡£çˆ¬è™«"""
        print("ğŸ”„ æ¨¡æ‹Ÿå­˜æ¡£çˆ¬è™«")
        headers = {
            'User-Agent': 'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "å­˜æ¡£çˆ¬è™«")
    
    def method_academic_crawler(self, url):
        """æ–¹æ³•8: æ¨¡æ‹Ÿå­¦æœ¯çˆ¬è™«"""
        print("ğŸ”„ æ¨¡æ‹Ÿå­¦æœ¯çˆ¬è™«")
        headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'From': 'research@university.edu'
        }
        response = self.session.get(url, headers=headers, timeout=15, verify=False)
        return self.process_response(response, "å­¦æœ¯çˆ¬è™«")
    
    def process_response(self, response, method_name):
        """å¤„ç†å“åº”"""
        if response.encoding is None:
            response.encoding = response.apparent_encoding or 'utf-8'
        
        content = response.text
        print(f"   ğŸ“Š {method_name}: çŠ¶æ€ç {response.status_code}, é•¿åº¦{len(content)}")
        print(f"   ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:100]}...")
        
        return {
            'method': method_name,
            'status_code': response.status_code,
            'content': content,
            'length': len(content),
            'headers': dict(response.headers)
        }
    
    def is_success(self, result):
        """åˆ¤æ–­æ˜¯å¦æˆåŠŸ"""
        content = result['content']
        
        # å¦‚æœå†…å®¹é•¿åº¦å¤ªçŸ­ï¼Œè®¤ä¸ºå¤±è´¥
        if len(content) < 100:
            return False
        
        # å¦‚æœåŒ…å«Access disabledï¼Œè®¤ä¸ºå¤±è´¥
        if "Access disabled" in content:
            return False
        
        # å¦‚æœæ˜¯ä¹±ç ï¼ˆåŒ…å«å¾ˆå¤šéASCIIå­—ç¬¦ï¼‰ï¼Œè®¤ä¸ºå¤±è´¥
        try:
            content.encode('ascii')
        except UnicodeEncodeError:
            non_ascii_ratio = sum(1 for c in content if ord(c) > 127) / len(content)
            if non_ascii_ratio > 0.3:  # å¦‚æœè¶…è¿‡30%æ˜¯éASCIIå­—ç¬¦
                return False
        
        # å¦‚æœåŒ…å«HTMLæ ‡ç­¾ï¼Œè®¤ä¸ºå¯èƒ½æˆåŠŸ
        if '<html' in content.lower() and '</html>' in content.lower():
            return True
        
        return False
    
    def extract_images_advanced(self, content, base_url):
        """é«˜çº§å›¾ç‰‡æå–"""
        if not content:
            return []
        
        print("ğŸ–¼ï¸ å¼€å§‹é«˜çº§å›¾ç‰‡æå–...")
        
        soup = BeautifulSoup(content, 'html.parser')
        image_urls = set()
        
        # 1. æ ‡å‡†imgæ ‡ç­¾
        for img in soup.find_all('img'):
            for attr in ['src', 'data-src', 'data-original', 'data-lazy', 'data-srcset']:
                src = img.get(attr)
                if src:
                    urls = self.extract_urls_from_srcset(src)
                    for url in urls:
                        full_url = urljoin(base_url, url)
                        if self.is_image_url(full_url):
                            image_urls.add(full_url)
        
        # 2. pictureæ ‡ç­¾
        for picture in soup.find_all('picture'):
            for source in picture.find_all('source'):
                srcset = source.get('srcset')
                if srcset:
                    urls = self.extract_urls_from_srcset(srcset)
                    for url in urls:
                        full_url = urljoin(base_url, url)
                        if self.is_image_url(full_url):
                            image_urls.add(full_url)
        
        # 3. CSSèƒŒæ™¯å›¾ç‰‡
        for element in soup.find_all(style=True):
            style = element.get('style', '')
            bg_matches = re.findall(r'background(?:-image)?\s*:\s*url\(["\']?([^"\']+)["\']?\)', style)
            for match in bg_matches:
                full_url = urljoin(base_url, match)
                if self.is_image_url(full_url):
                    image_urls.add(full_url)
        
        # 4. JavaScriptä¸­çš„å›¾ç‰‡URL
        for script in soup.find_all('script'):
            if script.string:
                js_matches = re.findall(r'["\']([^"\']*\.(?:jpg|jpeg|png|gif|bmp|webp|svg)[^"\']*)["\']', script.string, re.IGNORECASE)
                for match in js_matches:
                    full_url = urljoin(base_url, match)
                    if self.is_image_url(full_url):
                        image_urls.add(full_url)
        
        # 5. å…¨æ–‡æ­£åˆ™æœç´¢
        patterns = [
            r'https?://[^\s"\'<>]+\.(?:jpg|jpeg|png|gif|bmp|webp|svg)(?:\?[^\s"\'<>]*)?',
            r'["\']([^"\']+\.(?:jpg|jpeg|png|gif|bmp|webp|svg)(?:\?[^"\']*)?)["\']'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0]
                if match:
                    full_url = urljoin(base_url, match)
                    if self.is_image_url(full_url):
                        image_urls.add(full_url)
        
        result = list(image_urls)
        print(f"   ğŸ¯ æ€»å…±æ‰¾åˆ° {len(result)} ä¸ªå›¾ç‰‡URL")
        
        return result
    
    def extract_urls_from_srcset(self, srcset):
        """ä»srcsetä¸­æå–URL"""
        urls = []
        if srcset:
            # srcsetæ ¼å¼: "url1 1x, url2 2x" æˆ– "url1 100w, url2 200w"
            parts = srcset.split(',')
            for part in parts:
                url = part.strip().split()[0]
                if url:
                    urls.append(url)
        return urls
    
    def is_image_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡URL"""
        if not url or len(url) < 5:
            return False
        
        # è¿‡æ»¤æ‰æ˜æ˜¾çš„éå›¾ç‰‡URL
        if any(bad in url.lower() for bad in ['javascript:', 'mailto:', 'tel:', 'data:']):
            return False
        
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
        url_lower = url.lower()
        
        # æ£€æŸ¥æ‰©å±•å
        if any(ext in url_lower for ext in image_extensions):
            return True
        
        # æ£€æŸ¥å…³é”®è¯
        if any(keyword in url_lower for keyword in ['image', 'img', 'photo', 'pic', 'avatar', 'thumb']):
            return True
        
        return False
    
    def ultimate_crack(self, url):
        """ç»ˆæç ´è§£"""
        result = self.try_special_methods(url)
        
        if result:
            print(f"\nğŸ‰ æˆåŠŸç ´è§£! ä½¿ç”¨æ–¹æ³•: {result['method']}")
            
            # æå–å›¾ç‰‡
            images = self.extract_images_advanced(result['content'], url)
            
            return {
                'success': True,
                'method': result['method'],
                'status_code': result['status_code'],
                'content_length': result['length'],
                'images': images,
                'image_count': len(images),
                'content_preview': result['content'][:1000],
                'response_headers': result['headers']
            }
        else:
            return {
                'success': False,
                'message': 'æ‰€æœ‰ç ´è§£æ–¹æ³•éƒ½å¤±è´¥äº†',
                'images': []
            }

def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 ultimate_bypass.py <URL>")
        sys.exit(1)
    
    url = sys.argv[1]
    bypass = UltimateBypass()
    result = bypass.ultimate_crack(url)
    
    print("\n" + "=" * 60)
    print("ğŸ† ç»ˆæç ´è§£ç»“æœ:")
    print(f"   æˆåŠŸ: {result['success']}")
    
    if result['success']:
        print(f"   ç ´è§£æ–¹æ³•: {result['method']}")
        print(f"   çŠ¶æ€ç : {result['status_code']}")
        print(f"   å†…å®¹é•¿åº¦: {result['content_length']}")
        print(f"   æ‰¾åˆ°å›¾ç‰‡: {result['image_count']} å¼ ")
        
        if result['images']:
            print(f"   å›¾ç‰‡åˆ—è¡¨:")
            for i, img_url in enumerate(result['images'], 1):
                print(f"      {i}. {img_url}")
        
        print(f"\n   å“åº”å¤´:")
        for key, value in result['response_headers'].items():
            print(f"      {key}: {value}")
        
        print(f"\n   å†…å®¹é¢„è§ˆ:")
        print(f"   {result['content_preview']}")
    else:
        print(f"   å¤±è´¥åŸå› : {result['message']}")
    
    print(f"\nğŸ“‹ JSONç»“æœ:")
    # ç§»é™¤content_previewä»¥é¿å…è¾“å‡ºè¿‡é•¿
    output_result = result.copy()
    if 'content_preview' in output_result:
        output_result['content_preview'] = output_result['content_preview'][:200] + "..."
    print(json.dumps(output_result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main() 