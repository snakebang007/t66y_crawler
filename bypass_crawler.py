#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§ååçˆ¬è™«ç ´è§£å™¨
ä½¿ç”¨å¤šç§æŠ€æœ¯ç»•è¿‡ç½‘ç«™ä¿æŠ¤
"""

import requests
import time
import random
import json
import base64
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
import re
import sys
import os
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AdvancedBypassCrawler:
    """é«˜çº§ç»•è¿‡çˆ¬è™«"""
    
    def __init__(self):
        self.session = requests.Session()
        self.setup_advanced_session()
        self.cookies_jar = {}
        
    def setup_advanced_session(self):
        """è®¾ç½®é«˜çº§ä¼šè¯é…ç½®"""
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=5,
            backoff_factor=2,
            status_forcelist=[429, 500, 502, 503, 504, 520, 521, 522, 524],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # é«˜çº§æµè§ˆå™¨æŒ‡çº¹
        self.user_agents = [
            # Chrome æœ€æ–°ç‰ˆæœ¬
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            # Firefox æœ€æ–°ç‰ˆæœ¬
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0',
            # Safari
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
            # Edge
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
        ]
        
        # è®¾ç½®åŸºç¡€è¯·æ±‚å¤´
        self.base_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Sec-GPC': '1',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }
        
    def get_random_headers(self, referer=None):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        headers = self.base_headers.copy()
        headers['User-Agent'] = random.choice(self.user_agents)
        
        if referer:
            headers['Referer'] = referer
            
        # éšæœºæ·»åŠ ä¸€äº›å¯é€‰å¤´
        if random.choice([True, False]):
            headers['X-Requested-With'] = 'XMLHttpRequest'
            
        return headers
    
    def method_1_direct_access(self, url):
        """æ–¹æ³•1: ç›´æ¥è®¿é—®"""
        print("ğŸ”„ æ–¹æ³•1: ç›´æ¥è®¿é—®")
        try:
            headers = self.get_random_headers()
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            return self.process_response(response, "ç›´æ¥è®¿é—®")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_2_with_referer(self, url):
        """æ–¹æ³•2: å¸¦Refererè®¿é—®"""
        print("ğŸ”„ æ–¹æ³•2: å¸¦Refererè®¿é—®")
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            headers = self.get_random_headers(referer=base_url)
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            return self.process_response(response, "å¸¦Refererè®¿é—®")
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_3_step_by_step(self, url):
        """æ–¹æ³•3: åˆ†æ­¥è®¿é—®æ¨¡æ‹ŸçœŸå®ç”¨æˆ·"""
        print("ğŸ”„ æ–¹æ³•3: åˆ†æ­¥è®¿é—®æ¨¡æ‹ŸçœŸå®ç”¨æˆ·")
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            # æ­¥éª¤1: è®¿é—®ä¸»é¡µ
            print("   ğŸ“ æ­¥éª¤1: è®¿é—®ä¸»é¡µ")
            headers = self.get_random_headers()
            try:
                home_response = self.session.get(base_url, headers=headers, timeout=10, verify=False)
                time.sleep(random.uniform(2, 4))
                print(f"   âœ… ä¸»é¡µè®¿é—®æˆåŠŸ: {home_response.status_code}")
            except:
                print("   âš ï¸ ä¸»é¡µè®¿é—®å¤±è´¥ï¼Œç»§ç»­...")
            
            # æ­¥éª¤2: è®¿é—®robots.txt
            print("   ğŸ“ æ­¥éª¤2: è®¿é—®robots.txt")
            try:
                robots_headers = self.get_random_headers(referer=base_url)
                robots_response = self.session.get(f"{base_url}/robots.txt", headers=robots_headers, timeout=5, verify=False)
                time.sleep(random.uniform(1, 2))
                print(f"   âœ… robots.txtè®¿é—®: {robots_response.status_code}")
            except:
                print("   âš ï¸ robots.txtè®¿é—®å¤±è´¥ï¼Œç»§ç»­...")
            
            # æ­¥éª¤3: è®¿é—®favicon
            print("   ğŸ“ æ­¥éª¤3: è®¿é—®favicon")
            try:
                favicon_headers = self.get_random_headers(referer=base_url)
                favicon_response = self.session.get(f"{base_url}/favicon.ico", headers=favicon_headers, timeout=5, verify=False)
                time.sleep(random.uniform(1, 2))
                print(f"   âœ… faviconè®¿é—®: {favicon_response.status_code}")
            except:
                print("   âš ï¸ faviconè®¿é—®å¤±è´¥ï¼Œç»§ç»­...")
            
            # æ­¥éª¤4: è®¿é—®ç›®æ ‡é¡µé¢
            print("   ğŸ“ æ­¥éª¤4: è®¿é—®ç›®æ ‡é¡µé¢")
            target_headers = self.get_random_headers(referer=base_url)
            target_headers['Cache-Control'] = 'no-cache'
            target_headers['Pragma'] = 'no-cache'
            
            response = self.session.get(url, headers=target_headers, timeout=15, verify=False)
            return self.process_response(response, "åˆ†æ­¥è®¿é—®")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_4_js_simulation(self, url):
        """æ–¹æ³•4: æ¨¡æ‹ŸJavaScriptæ‰§è¡Œ"""
        print("ğŸ”„ æ–¹æ³•4: æ¨¡æ‹ŸJavaScriptæ‰§è¡Œ")
        try:
            parsed_url = urlparse(url)
            base_url = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            # å…ˆè·å–é¡µé¢
            headers = self.get_random_headers(referer=base_url)
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            
            if response.status_code == 200:
                # æ£€æŸ¥æ˜¯å¦æœ‰main.js
                if 'main.js' in response.text:
                    print("   ğŸ“ æ£€æµ‹åˆ°main.jsï¼Œå°è¯•è·å–...")
                    try:
                        js_headers = self.get_random_headers(referer=url)
                        js_headers['Accept'] = 'application/javascript, */*;q=0.1'
                        js_response = self.session.get(f"{base_url}/main.js", headers=js_headers, timeout=10, verify=False)
                        print(f"   âœ… main.jsè·å–: {js_response.status_code}")
                        
                        # ç­‰å¾…ä¸€æ®µæ—¶é—´æ¨¡æ‹ŸJSæ‰§è¡Œ
                        time.sleep(random.uniform(2, 5))
                        
                        # å†æ¬¡è¯·æ±‚ç›®æ ‡é¡µé¢
                        final_headers = self.get_random_headers(referer=url)
                        final_response = self.session.get(url, headers=final_headers, timeout=15, verify=False)
                        return self.process_response(final_response, "JSæ¨¡æ‹Ÿæ‰§è¡Œ")
                        
                    except Exception as js_e:
                        print(f"   âš ï¸ JSå¤„ç†å¤±è´¥: {js_e}")
            
            return self.process_response(response, "JSæ¨¡æ‹Ÿ")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_5_mobile_ua(self, url):
        """æ–¹æ³•5: ä½¿ç”¨ç§»åŠ¨ç«¯User-Agent"""
        print("ğŸ”„ æ–¹æ³•5: ä½¿ç”¨ç§»åŠ¨ç«¯User-Agent")
        try:
            mobile_uas = [
                'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
                'Mozilla/5.0 (Android 14; Mobile; rv:121.0) Gecko/121.0 Firefox/121.0',
                'Mozilla/5.0 (Linux; Android 14; SM-G998B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36'
            ]
            
            headers = self.base_headers.copy()
            headers['User-Agent'] = random.choice(mobile_uas)
            headers['sec-ch-ua-mobile'] = '?1'
            headers['sec-ch-ua-platform'] = '"Android"'
            
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            return self.process_response(response, "ç§»åŠ¨ç«¯UA")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_6_search_engine_bot(self, url):
        """æ–¹æ³•6: æ¨¡æ‹Ÿæœç´¢å¼•æ“çˆ¬è™«"""
        print("ğŸ”„ æ–¹æ³•6: æ¨¡æ‹Ÿæœç´¢å¼•æ“çˆ¬è™«")
        try:
            bot_uas = [
                'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)',
                'Mozilla/5.0 (compatible; Bingbot/2.0; +http://www.bing.com/bingbot.htm)',
                'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)'
            ]
            
            headers = {
                'User-Agent': random.choice(bot_uas),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            return self.process_response(response, "æœç´¢å¼•æ“Bot")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def method_7_curl_simulation(self, url):
        """æ–¹æ³•7: æ¨¡æ‹Ÿcurlè¯·æ±‚"""
        print("ğŸ”„ æ–¹æ³•7: æ¨¡æ‹Ÿcurlè¯·æ±‚")
        try:
            headers = {
                'User-Agent': 'curl/8.4.0',
                'Accept': '*/*',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(url, headers=headers, timeout=15, verify=False)
            return self.process_response(response, "curlæ¨¡æ‹Ÿ")
            
        except Exception as e:
            print(f"   âŒ å¤±è´¥: {e}")
            return None
    
    def process_response(self, response, method_name):
        """å¤„ç†å“åº”"""
        if response.status_code == 200:
            # ç¡®ä¿æ­£ç¡®è§£ç 
            if response.encoding is None:
                response.encoding = response.apparent_encoding or 'utf-8'
            
            content = response.text
            print(f"   âœ… {method_name}æˆåŠŸ: çŠ¶æ€ç {response.status_code}, å†…å®¹é•¿åº¦{len(content)}")
            print(f"   ğŸ“„ å†…å®¹é¢„è§ˆ: {content[:100]}...")
            
            # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
            if "Access disabled" in content:
                print(f"   âš ï¸ æ£€æµ‹åˆ°è®¿é—®è¢«æ‹’ç»")
                return None
            elif len(content) < 200:
                print(f"   âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œå¯èƒ½è¢«æ‹¦æˆª")
                return None
            else:
                print(f"   ğŸ‰ æˆåŠŸç»•è¿‡ä¿æŠ¤!")
                return content
        else:
            print(f"   âŒ {method_name}å¤±è´¥: çŠ¶æ€ç {response.status_code}")
            return None
    
    def extract_images_from_content(self, content, base_url):
        """ä»å†…å®¹ä¸­æå–å›¾ç‰‡"""
        if not content:
            return []
        
        soup = BeautifulSoup(content, 'html.parser')
        image_urls = set()
        
        print("ğŸ–¼ï¸ å¼€å§‹æå–å›¾ç‰‡...")
        
        # æ–¹æ³•1: imgæ ‡ç­¾
        img_tags = soup.find_all('img')
        print(f"   ğŸ“Š æ‰¾åˆ° {len(img_tags)} ä¸ªimgæ ‡ç­¾")
        
        for img in img_tags:
            src = img.get('src') or img.get('data-src') or img.get('data-original') or img.get('data-lazy')
            if src:
                full_url = urljoin(base_url, src)
                if self.is_image_url(full_url):
                    image_urls.add(full_url)
                    print(f"   ğŸ–¼ï¸ æ‰¾åˆ°å›¾ç‰‡: {full_url}")
        
        # æ–¹æ³•2: aæ ‡ç­¾ä¸­çš„å›¾ç‰‡é“¾æ¥
        a_tags = soup.find_all('a', href=True)
        image_links = 0
        for a in a_tags:
            href = a['href']
            if self.is_image_url(href):
                full_url = urljoin(base_url, href)
                image_urls.add(full_url)
                image_links += 1
                print(f"   ğŸ”— ä»é“¾æ¥æ‰¾åˆ°å›¾ç‰‡: {full_url}")
        
        print(f"   ğŸ“Š ä»aæ ‡ç­¾æ‰¾åˆ° {image_links} ä¸ªå›¾ç‰‡é“¾æ¥")
        
        # æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼æœç´¢
        patterns = [
            r'https?://[^\s"\'<>]+\.(?:jpg|jpeg|png|gif|bmp|webp|svg)',
            r'["\']([^"\']+\.(?:jpg|jpeg|png|gif|bmp|webp|svg))["\']',
            r'src\s*=\s*["\']([^"\']+\.(?:jpg|jpeg|png|gif|bmp|webp|svg))["\']',
            r'data-src\s*=\s*["\']([^"\']+\.(?:jpg|jpeg|png|gif|bmp|webp|svg))["\']'
        ]
        
        regex_found = 0
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    match = match[0] if match[0] else ''
                if match:
                    full_url = urljoin(base_url, match)
                    if self.is_image_url(full_url) and full_url not in image_urls:
                        image_urls.add(full_url)
                        regex_found += 1
                        print(f"   ğŸ” æ­£åˆ™åŒ¹é…æ‰¾åˆ°: {full_url}")
        
        print(f"   ğŸ“Š æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ° {regex_found} ä¸ªæ–°å›¾ç‰‡")
        
        # æ–¹æ³•4: CSSèƒŒæ™¯å›¾ç‰‡
        style_elements = soup.find_all(style=True)
        bg_found = 0
        for element in style_elements:
            style = element.get('style', '')
            bg_matches = re.findall(r'background-image:\s*url\(["\']?([^"\']+)["\']?\)', style)
            for bg_match in bg_matches:
                full_url = urljoin(base_url, bg_match)
                if self.is_image_url(full_url) and full_url not in image_urls:
                    image_urls.add(full_url)
                    bg_found += 1
                    print(f"   ğŸ¨ CSSèƒŒæ™¯å›¾ç‰‡: {full_url}")
        
        print(f"   ğŸ“Š CSSèƒŒæ™¯å›¾ç‰‡æ‰¾åˆ° {bg_found} ä¸ª")
        
        return list(image_urls)
    
    def is_image_url(self, url):
        """æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡URL"""
        if not url or len(url) < 5:
            return False
        
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.svg']
        url_lower = url.lower()
        
        # æ£€æŸ¥æ‰©å±•å
        if any(ext in url_lower for ext in image_extensions):
            return True
        
        # æ£€æŸ¥å…³é”®è¯
        if any(keyword in url_lower for keyword in ['image', 'img', 'photo', 'pic', 'avatar']):
            return True
        
        return False
    
    def crack_website(self, url):
        """ç ´è§£ç½‘ç«™"""
        print(f"ğŸ¯ å¼€å§‹ç ´è§£ç½‘ç«™: {url}")
        print("=" * 60)
        
        # å°è¯•æ‰€æœ‰æ–¹æ³•
        methods = [
            self.method_1_direct_access,
            self.method_2_with_referer,
            self.method_3_step_by_step,
            self.method_4_js_simulation,
            self.method_5_mobile_ua,
            self.method_6_search_engine_bot,
            self.method_7_curl_simulation
        ]
        
        for i, method in enumerate(methods, 1):
            print(f"\nğŸ”§ å°è¯•æ–¹æ³• {i}/{len(methods)}")
            content = method(url)
            
            if content:
                print(f"ğŸ‰ æ–¹æ³• {i} æˆåŠŸç ´è§£!")
                
                # æå–å›¾ç‰‡
                images = self.extract_images_from_content(content, url)
                
                return {
                    'success': True,
                    'method': f"æ–¹æ³•{i}: {method.__name__}",
                    'content_length': len(content),
                    'images': images,
                    'image_count': len(images),
                    'content_preview': content[:500]
                }
            
            # æ·»åŠ éšæœºå»¶æ—¶é¿å…è¢«æ£€æµ‹
            time.sleep(random.uniform(1, 3))
        
        print("\nâŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        return {
            'success': False,
            'message': 'æ— æ³•ç ´è§£ç½‘ç«™ä¿æŠ¤',
            'images': []
        }

def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python3 bypass_crawler.py <URL>")
        sys.exit(1)
    
    url = sys.argv[1]
    crawler = AdvancedBypassCrawler()
    result = crawler.crack_website(url)
    
    print("\n" + "=" * 60)
    print("ğŸ† æœ€ç»ˆç»“æœ:")
    print(f"   æˆåŠŸ: {result['success']}")
    
    if result['success']:
        print(f"   ç ´è§£æ–¹æ³•: {result['method']}")
        print(f"   å†…å®¹é•¿åº¦: {result['content_length']}")
        print(f"   æ‰¾åˆ°å›¾ç‰‡: {result['image_count']} å¼ ")
        
        if result['images']:
            print(f"   å›¾ç‰‡åˆ—è¡¨:")
            for i, img_url in enumerate(result['images'], 1):
                print(f"      {i}. {img_url}")
        
        print(f"\n   å†…å®¹é¢„è§ˆ:")
        print(f"   {result['content_preview']}")
    else:
        print(f"   å¤±è´¥åŸå› : {result['message']}")
    
    # è¾“å‡ºJSONç»“æœ
    print(f"\nğŸ“‹ JSONç»“æœ:")
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    main() 